(window.webpackJsonp=window.webpackJsonp||[]).push([[67],{206:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return c})),n.d(t,"rightToc",(function(){return l})),n.d(t,"default",(function(){return p}));var a=n(1),o=n(9),r=(n(0),n(441)),i={},c={id:"jme3/advanced/capture_audio_video_to_a_file",title:"capture_audio_video_to_a_file",description:"So you've made your cool new JMonkeyEngine3 game and you want to create",source:"@site/docs/jme3/advanced/capture_audio_video_to_a_file.md",permalink:"/wikidemo/docs/jme3/advanced/capture_audio_video_to_a_file",editUrl:"https://github.com/8keep/wikidemo/edit/master/docs/jme3/advanced/capture_audio_video_to_a_file.md"},l=[{value:"Basic Example",id:"basic-example",children:[]},{value:"How it works",id:"how-it-works",children:[]},{value:"Advanced Example",id:"advanced-example",children:[]},{value:"Using Advanced features to Record from more than one perspective at once",id:"using-advanced-features-to-record-from-more-than-one-perspective-at-once",children:[]}],s={rightToc:l};function p(e){var t=e.components,n=Object(o.a)(e,["components"]);return Object(r.b)("wrapper",Object(a.a)({},s,n,{components:t,mdxType:"MDXLayout"}),Object(r.b)("p",null,"So you've made your cool new JMonkeyEngine3 game and you want to create\na demo video to show off your hard work. Or maybe you want to make a\ncutscene for your game using the physics and characters in the game\nitself. Screen capturing is the most straightforward way to do this, but\nit can slow down your game and produce low-quality video and audio as a\nresult. A better way is to record video and audio directly from the game\nwhile it is running using VideoRecorderAppState."),Object(r.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(r.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-heading"}),Object(r.b)("h5",{parentName:"div"},Object(r.b)("span",Object(a.a)({parentName:"h5"},{className:"admonition-icon"}),Object(r.b)("svg",Object(a.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(r.b)("path",Object(a.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(r.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-content"}),Object(r.b)("p",{parentName:"div"},"Combine this method with jMonkeyEngine's\n",Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"../../jme3/advanced/cinematics"}),"Cinematics")," feature to record\nhigh-quality game trailers!"))),Object(r.b)("h1",{id:"simple-way"},"Simple Way"),Object(r.b)("p",null,"First off, if all you need is to record video at 30fps with no sound,\nthen look no further than jMonkeyEngine 3's built in\n",Object(r.b)("inlineCode",{parentName:"p"},"VideoRecorderAppState")," class."),Object(r.b)("p",null,"Add the following code to your ",Object(r.b)("inlineCode",{parentName:"p"},"simpleInitApp()")," method."),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{className:"language-java"}),"stateManager.attach(new VideoRecorderAppState()); //start recording\n")),Object(r.b)("p",null,"The game will run slow, but the recording will be in high-quality and\nnormal speed. Recording starts when the state is attached, and ends when\nthe application quits or the state is detached."),Object(r.b)("p",null,"The video files will be stored in your ",Object(r.b)("strong",{parentName:"p"},"user home directory"),". If you\nwant to save to another path, specify a File object in the\nVideoRecorderAppState constructor."),Object(r.b)("p",null,"That's all!"),Object(r.b)("h1",{id:"advanced-way"},"Advanced Way"),Object(r.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(r.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-heading"}),Object(r.b)("h5",{parentName:"div"},Object(r.b)("span",Object(a.a)({parentName:"h5"},{className:"admonition-icon"}),Object(r.b)("svg",Object(a.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(r.b)("path",Object(a.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(r.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-content"}),Object(r.b)("p",{parentName:"div"},"This way of A/V recording is still in development. It works for all of\njMonkeyEngine's test cases. If you experience any problems or if\nsomething isn't clear, please ",Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://jmonkeyengine.org/members/bortreb/"}),"let me\nknow"),". -- bortreb"))),Object(r.b)("p",null,"If you want to record audio as well, record at different framerates, or\nrecord from multiple viewpoints at once, then there's a full solution\nfor doing this already made for you here:"),Object(r.b)("p",null,Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://www.aurellem.com/releases/jmeCapture-latest.zip"}),"http://www.aurellem.com/releases/jmeCapture-latest.zip")),Object(r.b)("p",null,Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://www.aurellem.com/releases/jmeCapture-latest.tar.bz2"}),"http://www.aurellem.com/releases/jmeCapture-latest.tar.bz2")),Object(r.b)("p",null,"Download the archive in your preferred format, extract, add the jars to\nyour project, and you are ready to go."),Object(r.b)("p",null,"The javadoc is here: ",Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://www.aurellem.com/jmeCapture/docs/"}),"http://www.aurellem.com/jmeCapture/docs/")),Object(r.b)("p",null,"To capture video and audio you use the ",Object(r.b)("inlineCode",{parentName:"p"},"com.aurellem.capture.Capture"),"\nclass, which has two methods, ",Object(r.b)("inlineCode",{parentName:"p"},"captureAudio()")," and ",Object(r.b)("inlineCode",{parentName:"p"},"captureVideo()"),", and\nthe ",Object(r.b)("inlineCode",{parentName:"p"},"com.aurellem.capture.IsoTimer")," class, which sets the audio and\nvideo framerate."),Object(r.b)("p",null,"The steps are:"),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{className:"language-java"}),"yourApp.setTimer(new IsoTimer(desiredFramesPerSecond));\n")),Object(r.b)("p",null,"This causes jMonkeyEngine to take as much time as it needs to fully\ncalculate every frame of the video and audio. You will see your game\nspeed up and slow down depending on how computationally demanding your\ngame is, but the final recorded audio and video will be perfectly\nsychronized and will run at exactly the fps which you specified."),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{className:"language-java"}),"captureVideo(yourApp, targetVideoFile);\ncaptureAudio(yourApp, targetAudioFile);\n")),Object(r.b)("p",null,"These will cause the app to record audio and video when it is run. Audio\nand video will stop being recorded when the app stops. Your audio will\nbe recorded as a 44,100 Hz linear PCM wav file, while the video will be\nrecorded according to the following rules:"),Object(r.b)("p",null,"1.) (Preferred) If you supply an empty directory as the file, then the\nvideo will be saved as a sequence of .png files, one file per frame. The\nfiles start at 0000000.png and increment from there. You can then\ncombine the frames into your preferred container/codec. If the directory\nis not empty, then writing video frames to it will fail, and nothing\nwill be written."),Object(r.b)("p",null,'2.) If the filename ends in ".avi then the frames will be encoded as a\nRAW stream inside an AVI 1.0 container. The resulting file will be quite\nlarge and you will probably want to re-encode it to your preferred\ncontainer/codec format. Be advised that some video payers cannot process\nAVI with a RAW stream, and that AVI 1.0 files generated by this method\nthat exceed 2.0GB are invalid according to the AVI 1.0 spec (but many\nprograms can still deal with them.) Thanks to ',Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://www.randelshofer.ch/blog/2008/08/writing-avi-videos-in-pure-java/"}),"Werner\nRandelshofer"),"\nfor his excellent work which made the AVI file writer option possible."),Object(r.b)("p",null,'3.) Any non-directory file ending in anything other than ".avi will be\nprocessed through Xuggle. Xuggle provides the option to use many\ncodecs/containers, but you will have to install it on your system\nyourself in order to use this option. Please visit\n',Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://www.xuggle.com/"}),"http://www.xuggle.com/")," to learn how to do this."),Object(r.b)("p",null,"Note that you will not hear any sound if you choose to record sound to a\nfile."),Object(r.b)("h2",{id:"basic-example"},"Basic Example"),Object(r.b)("p",null,"Here is a complete example showing how to capture both audio and video\nfrom one of jMonkeyEngine3's advanced demo applications."),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{className:"language-java"}),'import java.io.File;\nimport java.io.IOException;\n\nimport jme3test.water.TestPostWater;\n\nimport com.aurellem.capture.Capture;\nimport com.aurellem.capture.IsoTimer;\nimport com.jme3.app.SimpleApplication;\n\n\n/**\n * Demonstrates how to use basic Audio/Video capture with a\n * jMonkeyEngine application. You can use these techniques to make\n * high quality cutscenes or demo videos, even on very slow laptops.\n *\n * @author Robert McIntyre\n */\n\npublic class Basic {\n\n    public static void main(String[] ignore) throws IOException{\n    File video = File.createTempFile("JME-water-video", ".avi");\n    File audio = File.createTempFile("JME-water-audio", ".wav");\n\n    SimpleApplication app = new TestPostWater();\n    app.setTimer(new IsoTimer(60));\n    app.setShowSettings(false);\n\n    Capture.captureVideo(app, video);\n    Capture.captureAudio(app, audio);\n\n    app.start();\n\n    System.out.println(video.getCanonicalPath());\n    System.out.println(audio.getCanonicalPath());\n    }\n}\n')),Object(r.b)("h2",{id:"how-it-works"},"How it works"),Object(r.b)("p",null,"A standard JME3 application that extends ",Object(r.b)("inlineCode",{parentName:"p"},"SimpleApplication")," or\n",Object(r.b)("inlineCode",{parentName:"p"},"Application")," tries as hard as it can to keep in sync with ",Object(r.b)("em",{parentName:"p"},"user-time"),".\nIf a ball is rolling at 1 game-mile per game-hour in the game, and you\nwait for one user-hour as measured by the clock on your wall, then the\nball should have traveled exactly one game-mile. In order to keep sync\nwith the real world, the game throttles its physics engine and graphics\ndisplay. If the computations involved in running the game are too\nintense, then the game will first skip frames, then sacrifice physics\naccuracy. If there are particuraly demanding computations, then you may\nonly get 1 fps, and the ball may tunnel through the floor or obstacles\ndue to inaccurate physics simulation, but after the end of one\nuser-hour, that ball will have traveled one game-mile."),Object(r.b)("p",null,"When we're recording video, we don't care if the game-time syncs with\nuser-time, but instead whether the time in the recorded video\n(video-time) syncs with user-time. To continue the analogy, if we\nrecorded the ball rolling at 1 game-mile per game-hour and watched the\nvideo later, we would want to see 30 fps video of the ball rolling at 1\nvideo-mile per ",Object(r.b)("em",{parentName:"p"},"user-hour"),". It doesn't matter how much user-time it took\nto simulate that hour of game-time to make the high-quality recording."),Object(r.b)("p",null,"The IsoTimer ignores real-time and always reports that the same amount\nof time has passed every time it is called. That way, one can put code\nto write each video/audio frame to a file without worrying about that\ncode itself slowing down the game to the point where the recording would\nbe useless."),Object(r.b)("h2",{id:"advanced-example"},"Advanced Example"),Object(r.b)("p",null,"The package from aurellem.com was made for AI research and can do more\nthan just record a single stream of audio and video. You can use it to:"),Object(r.b)("p",null,"1.) Create multiple independent listeners that each hear the world from\ntheir own perspective."),Object(r.b)("p",null,"2.) Process the sound data in any way you wish."),Object(r.b)("p",null,"3.) Do the same for visual data."),Object(r.b)("p",null,"Here is a more advanced example, which can also be found along with\nother examples in the jmeCapture.jar file included in the distribution."),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{className:"language-java"}),'package com.aurellem.capture.examples;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.lang.reflect.Field;\nimport java.nio.ByteBuffer;\n\nimport javax.sound.sampled.AudioFormat;\n\nimport org.tritonus.share.sampled.FloatSampleTools;\n\nimport com.aurellem.capture.AurellemSystemDelegate;\nimport com.aurellem.capture.Capture;\nimport com.aurellem.capture.IsoTimer;\nimport com.aurellem.capture.audio.CompositeSoundProcessor;\nimport com.aurellem.capture.audio.MultiListener;\nimport com.aurellem.capture.audio.SoundProcessor;\nimport com.aurellem.capture.audio.WaveFileWriter;\nimport com.jme3.app.SimpleApplication;\nimport com.jme3.audio.AudioNode;\nimport com.jme3.audio.Listener;\nimport com.jme3.cinematic.MotionPath;\nimport com.jme3.cinematic.events.AbstractCinematicEvent;\nimport com.jme3.cinematic.events.MotionTrack;\nimport com.jme3.material.Material;\nimport com.jme3.math.ColorRGBA;\nimport com.jme3.math.FastMath;\nimport com.jme3.math.Quaternion;\nimport com.jme3.math.Vector3f;\nimport com.jme3.scene.Geometry;\nimport com.jme3.scene.Node;\nimport com.jme3.scene.shape.Box;\nimport com.jme3.scene.shape.Sphere;\nimport com.jme3.system.AppSettings;\nimport com.jme3.system.JmeSystem;\n\n/**\n *\n * Demonstrates advanced use of the audio capture and recording\n * features.  Multiple perspectives of the same scene are\n * simultaneously rendered to different sound files.\n *\n * A key limitation of the way multiple listeners are implemented is\n * that only 3D positioning effects are realized for listeners other\n * than the main LWJGL listener.  This means that audio effects such\n * as environment settings will *not* be heard on any auxiliary\n * listeners, though sound attenuation will work correctly.\n *\n * Multiple listeners as realized here might be used to make AI\n * entities that can each hear the world from their own perspective.\n *\n * @author Robert McIntyre\n */\n\npublic class Advanced extends SimpleApplication {\n\n    /**\n     * You will see three grey cubes, a blue sphere, and a path which\n     * circles each cube.  The blue sphere is generating a constant\n     * monotone sound as it moves along the track.  Each cube is\n     * listening for sound; when a cube hears sound whose intensity is\n     * greater than a certain threshold, it changes its color from\n     * grey to green.\n     *\n     *  Each cube is also saving whatever it hears to a file.  The\n     *  scene from the perspective of the viewer is also saved to a\n     *  video file.  When you listen to each of the sound files\n     *  alongside the video, the sound will get louder when the sphere\n     *  approaches the cube that generated that sound file.  This\n     *  shows that each listener is hearing the world from its own\n     *  perspective.\n     *\n     */\n    public static void main(String[] args) {\n        Advanced app = new Advanced();\n        AppSettings settings = new AppSettings(true);\n        settings.setAudioRenderer(AurellemSystemDelegate.SEND);\n        JmeSystem.setSystemDelegate(new AurellemSystemDelegate());\n        app.setSettings(settings);\n        app.setShowSettings(false);\n        app.setPauseOnLostFocus(false);\n\n\n        try {\n            Capture.captureVideo(app, File.createTempFile("advanced",".avi"));\n            Capture.captureAudio(app, File.createTempFile("advanced", ".wav"));\n        }\n        catch (IOException e) {e.printStackTrace();}\n\n        app.start();\n    }\n\n\n    private Geometry bell;\n    private Geometry ear1;\n    private Geometry ear2;\n    private Geometry ear3;\n    private AudioNode music;\n    private MotionTrack motionControl;\n    private IsoTimer motionTimer = new IsoTimer(60);\n\n    private Geometry makeEar(Node root, Vector3f position){\n        Material mat = new Material(assetManager, "Common/MatDefs/Misc/Unshaded.j3md");\n        Geometry ear = new Geometry("ear", new Box(1.0f, 1.0f, 1.0f));\n        ear.setLocalTranslation(position);\n        mat.setColor("Color", ColorRGBA.Green);\n        ear.setMaterial(mat);\n        root.attachChild(ear);\n        return ear;\n    }\n\n    private Vector3f[] path = new Vector3f[]{\n            // loop 1\n            new Vector3f(0, 0, 0),\n            new Vector3f(0, 0, -10),\n            new Vector3f(-2, 0, -14),\n            new Vector3f(-6, 0, -20),\n            new Vector3f(0, 0, -26),\n            new Vector3f(6, 0, -20),\n            new Vector3f(0, 0, -14),\n            new Vector3f(-6, 0, -20),\n            new Vector3f(0, 0, -26),\n            new Vector3f(6, 0, -20),\n            // loop 2\n            new Vector3f(5, 0, -5),\n            new Vector3f(7, 0, 1.5f),\n            new Vector3f(14, 0, 2),\n            new Vector3f(20, 0, 6),\n            new Vector3f(26, 0, 0),\n            new Vector3f(20, 0, -6),\n            new Vector3f(14, 0, 0),\n            new Vector3f(20, 0, 6),\n            new Vector3f(26, 0, 0),\n            new Vector3f(20, 0, -6),\n            new Vector3f(14, 0, 0),\n            // loop 3\n            new Vector3f(8, 0, 7.5f),\n            new Vector3f(7, 0, 10.5f),\n            new Vector3f(6, 0, 20),\n            new Vector3f(0, 0, 26),\n            new Vector3f(-6, 0, 20),\n            new Vector3f(0, 0, 14),\n            new Vector3f(6, 0, 20),\n            new Vector3f(0, 0, 26),\n            new Vector3f(-6, 0, 20),\n            new Vector3f(0, 0, 14),\n            // begin ellipse\n            new Vector3f(16, 5, 20),\n            new Vector3f(0, 0, 26),\n            new Vector3f(-16, -10, 20),\n            new Vector3f(0, 0, 14),\n            new Vector3f(16, 20, 20),\n            new Vector3f(0, 0, 26),\n            new Vector3f(-10, -25, 10),\n            new Vector3f(-10, 0, 0),\n            // come at me!\n            new Vector3f(-28.00242f, 48.005623f, -34.648228f),\n            new Vector3f(0, 0 , -20),\n    };\n\n    private void createScene() {\n        Material mat = new Material(assetManager, "Common/MatDefs/Misc/Unshaded.j3md");\n        bell = new Geometry( "sound-emitter" , new Sphere(15,15,1));\n        mat.setColor("Color", ColorRGBA.Blue);\n        bell.setMaterial(mat);\n        rootNode.attachChild(bell);\n\n        ear1 = makeEar(rootNode, new Vector3f(0, 0 ,-20));\n        ear2 = makeEar(rootNode, new Vector3f(0, 0 ,20));\n        ear3 = makeEar(rootNode, new Vector3f(20, 0 ,0));\n\n        MotionPath track = new MotionPath();\n\n        for (Vector3f v : path){\n            track.addWayPoint(v);\n        }\n        track.setCurveTension(0.80f);\n\n        motionControl = new MotionTrack(bell,track);\n        // for now, use reflection to change the timer...\n        // motionControl.setTimer(new IsoTimer(60));\n\n        try {\n            Field timerField;\n            timerField = AbstractCinematicEvent.class.getDeclaredField("timer");\n            timerField.setAccessible(true);\n            try {timerField.set(motionControl, motionTimer);}\n            catch (IllegalArgumentException e) {e.printStackTrace();}\n            catch (IllegalAccessException e) {e.printStackTrace();}\n        }\n        catch (SecurityException e) {e.printStackTrace();}\n        catch (NoSuchFieldException e) {e.printStackTrace();}\n\n\n        motionControl.setDirectionType(MotionTrack.Direction.PathAndRotation);\n        motionControl.setRotation(new Quaternion().fromAngleNormalAxis(-FastMath.HALF_PI, Vector3f.UNIT_Y));\n        motionControl.setInitialDuration(20f);\n        motionControl.setSpeed(1f);\n\n        track.enableDebugShape(assetManager, rootNode);\n        positionCamera();\n    }\n\n\n    private void positionCamera(){\n        this.cam.setLocation(new Vector3f(-28.00242f, 48.005623f, -34.648228f));\n        this.cam.setRotation(new Quaternion(0.3359635f, 0.34280345f, -0.13281013f, 0.8671653f));\n    }\n\n    private void initAudio() {\n        org.lwjgl.input.Mouse.setGrabbed(false);\n        music = new AudioNode(assetManager, "Sound/Effects/Beep.ogg", false);\n\n        rootNode.attachChild(music);\n        audioRenderer.playSource(music);\n        music.setPositional(true);\n        music.setVolume(1f);\n        music.setReverbEnabled(false);\n        music.setDirectional(false);\n        music.setMaxDistance(200.0f);\n        music.setRefDistance(1f);\n        //music.setRolloffFactor(1f);\n        music.setLooping(false);\n        audioRenderer.pauseSource(music);\n    }\n\n    public class Dancer implements SoundProcessor {\n        Geometry entity;\n        float scale = 2;\n        public Dancer(Geometry entity){\n            this.entity = entity;\n        }\n\n        /**\n         * this method is irrelevant since there is no state to cleanup.\n         */\n        public void cleanup() {}\n\n\n        /**\n         * Respond to sound!  This is the brain of an AI entity that\n         * hears its surroundings and reacts to them.\n         */\n        public void process(ByteBuffer audioSamples, int numSamples, AudioFormat format) {\n            audioSamples.clear();\n            byte[] data = new byte[numSamples];\n            float[] out = new float[numSamples];\n            audioSamples.get(data);\n            FloatSampleTools.byte2floatInterleaved(data, 0, out, 0,\n                    numSamples/format.getFrameSize(), format);\n\n            float max = Float.NEGATIVE_INFINITY;\n            for (float f : out){if (f > max) max = f;}\n            audioSamples.clear();\n\n            if (max > 0.1){entity.getMaterial().setColor("Color", ColorRGBA.Green);}\n            else {entity.getMaterial().setColor("Color", ColorRGBA.Gray);}\n        }\n    }\n\n    private void prepareEar(Geometry ear, int n){\n        if (this.audioRenderer instanceof MultiListener){\n            MultiListener rf = (MultiListener)this.audioRenderer;\n\n            Listener auxListener = new Listener();\n            auxListener.setLocation(ear.getLocalTranslation());\n\n            rf.addListener(auxListener);\n            WaveFileWriter aux = null;\n\n            try {aux = new WaveFileWriter(File.createTempFile("advanced-audio-" + n, ".wav"));}\n            catch (IOException e) {e.printStackTrace();}\n\n            rf.registerSoundProcessor(auxListener,\n                    new CompositeSoundProcessor(new Dancer(ear), aux));\n\n        }\n    }\n\n\n    public void simpleInitApp() {\n        this.setTimer(new IsoTimer(60));\n        initAudio();\n\n        createScene();\n\n        prepareEar(ear1, 1);\n        prepareEar(ear2, 1);\n        prepareEar(ear3, 1);\n\n        motionControl.play();\n\n    }\n\n    public void simpleUpdate(float tpf) {\n        motionTimer.update();\n        if (music.getStatus() != AudioSource.Status.Playing){\n            music.play();\n        }\n        Vector3f loc = cam.getLocation();\n        Quaternion rot = cam.getRotation();\n        listener.setLocation(loc);\n        listener.setRotation(rot);\n        music.setLocalTranslation(bell.getLocalTranslation());\n    }\n\n}\n')),Object(r.b)("p",null,Object(r.b)("img",Object(a.a)({parentName:"p"},{src:"http://www.youtube.com/v/oCEfK0yhDrY?.swf",alt:"oCEfK0yhDrY?.swf"}))),Object(r.b)("h2",{id:"using-advanced-features-to-record-from-more-than-one-perspective-at-once"},"Using Advanced features to Record from more than one perspective at once"),Object(r.b)("p",null,Object(r.b)("img",Object(a.a)({parentName:"p"},{src:"http://www.youtube.com/v/WIJt9aRGusc?.swf",alt:"WIJt9aRGusc?.swf"}))),Object(r.b)("h1",{id:"more-information"},"More Information"),Object(r.b)("p",null,"This is the old page showing the first version of this idea\n",Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://aurellem.org/cortex/html/capture-video.html"}),"http://aurellem.org/cortex/html/capture-video.html")),Object(r.b)("p",null,"All source code can be found here:"),Object(r.b)("p",null,Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://hg.bortreb.com/audio-send"}),"http://hg.bortreb.com/audio-send")),Object(r.b)("p",null,Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://hg.bortreb.com/jmeCapture"}),"http://hg.bortreb.com/jmeCapture")),Object(r.b)("p",null,"More information on the modifications to OpenAL to support multiple\nlisteners can be found here."),Object(r.b)("p",null,Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://aurellem.org/audio-send/html/ear.html"}),"http://aurellem.org/audio-send/html/ear.html")))}p.isMDXComponent=!0},441:function(e,t,n){"use strict";n.d(t,"a",(function(){return m})),n.d(t,"b",(function(){return h}));var a=n(0),o=n.n(a);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function c(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=o.a.createContext({}),p=function(e){var t=o.a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):c({},t,{},e)),n},m=function(e){var t=p(e.components);return o.a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.a.createElement(o.a.Fragment,{},t)}},u=Object(a.forwardRef)((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,i=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=a,h=m["".concat(i,".").concat(u)]||m[u]||d[u]||r;return n?o.a.createElement(h,c({ref:t},s,{components:n})):o.a.createElement(h,c({ref:t},s))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=u;var c={};for(var l in t)hasOwnProperty.call(t,l)&&(c[l]=t[l]);c.originalType=e,c.mdxType="string"==typeof e?e:a,i[1]=c;for(var s=2;s<r;s++)i[s]=n[s];return o.a.createElement.apply(null,i)}return o.a.createElement.apply(null,n)}u.displayName="MDXCreateElement"}}]);